{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## **Overview**\n",
    "**we will explore how to develop a suite of different types of LSTM models for time series forecasting.**\n",
    "\n",
    "* ### **Univariate LSTM Models**\n",
    "\n",
    "1. Data Preparation\n",
    "2. Vanilla LSTM\n",
    "3. Stacked LSTM\n",
    "4. Bidirectional LSTM\n",
    "5. CNN LSTM\n",
    "6. ConvLSTM\n",
    "\n",
    "\n",
    "* ### **Multivariate LSTM Models**\n",
    "\n",
    "* ### **Multi-Step LSTM Models**\n",
    "\n",
    "* ### **Multivariate Multi-Step LSTM Models**\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import warnings\n",
    "# 忽略警告\n",
    "warnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score as r2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from scikeras.wrappers import KerasRegressor # 回归神经网络\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "# univariate bidirectional lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import CuDNNLSTM\n",
    "from keras.layers import Bidirectional\n",
    "import keras.backend as K\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from keras import optimizers\n",
    "from pygame import mixer\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import r2_score as r2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# def read_someone_day_file(day):\n",
    "#     IMF = []\n",
    "#     xlsx_file = '../../Data/CEEMDAN/SingleFactor/558606/'\n",
    "#     xlsx_path = glob.glob(os.path.join(xlsx_file,'*.xlsx'))\n",
    "#\n",
    "#     # data_col_len = pd.read_excel('../../Data/CEEMDAN/SingleFactor/558606/CEEMDAN_58606 Station_g.xlsx',header=None)\n",
    "#\n",
    "#     for imf_len in range(len(data_col_len.columns)):\n",
    "#         for file in xlsx_path:\n",
    "#              print(file)\n",
    "#              # print(os.path.basename(file))\n",
    "#              imfs = pd.read_excel(file,header=None)\n",
    "#              imfn = imfs.iloc[:,imf_len]\n",
    "#              imfn.to_numpy()\n",
    "#              IMF.append(imfn )\n",
    "#         print(imf_len,\"-------------------\")\n",
    "#     return IMF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def Result_Ana(Model, feature_num,testX,testY,scaler):\n",
    "    print('Result_Ana_Function:',testX.shape)\n",
    "    y_pred = Model.predict(testX)\n",
    "    y_pred = np.array(y_pred).reshape(-1, 1)\n",
    "    # 将一个数组prediction在最后一个轴上（即axis=-1）进行复制，重复8次，并将结果存储在prediction_copies_array数组中\n",
    "    # prediction_copies_array = np.repeat(y_pred, feature_num,axis=-1)\n",
    "    # prediction_copies_array\n",
    "    # print('-----------prediction copy Shape：',prediction_copies_array.shape,'------------------')\n",
    "    # Multi Var prediction\n",
    "    pred = scaler.inverse_transform(y_pred) #进行逆变换但是，只需要最后一列\n",
    "    # Single prediction\n",
    "    # pred = scaler.inverse_transform(y_pred) #进行逆变换但是，只需要最后一列\n",
    "\n",
    "    # print(pred)\n",
    "    print('testY of shape :',testY.shape)\n",
    "    y_true = np.array(testY).reshape(-1, 1)\n",
    "    # original_copies_array = np.repeat(testY, feature_num, axis=-1)\n",
    "    # print('IMF_True:',scaler.inverse_transform(np.reshape(original_copies_array, (len(testY), feature_num))))\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "\n",
    "    # print('feature_num-1_ETO_true:',y_true)\n",
    "\n",
    "    # Single true value\n",
    "    # y_true = scaler.inverse_transform(np.array(testY).reshape(-1,1))\n",
    "\n",
    "    # y_true\n",
    "    # pred\n",
    "\n",
    "    # print(pred)\n",
    "    plt.plot(y_true, color='red', label='Real Value')\n",
    "    plt.plot(pred, color='blue', label='Pred Value')\n",
    "    plt.title('Prediction ETO (mm)')\n",
    "    plt.xlabel('Time (day)')\n",
    "    plt.ylabel('ETO (mm)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print('MSE:',mse(y_true,pred))\n",
    "    print('MAE:',mae(y_true,pred))\n",
    "    print('R²:',r2(y_true,pred))\n",
    "    print('RMSE:',np.sqrt(mse(y_true,pred)))\n",
    "    # print('pred_finally:',pred)\n",
    "\n",
    "    return y_true,pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "IMF = pd.read_excel('../../Data/CEEMDAN/SingleFactor/558606/'+'CEEMDAN_58606 Station_'+'nine'+'.xlsx',header=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0     0.079223  0.106928 -0.080420 -0.556599 -0.048314 -0.451930 -0.209208   \n1     0.167664  0.061526 -0.061378 -0.577853 -0.040365 -0.462905 -0.213679   \n2    -0.110238 -0.007235 -0.102145 -0.605239 -0.012286 -0.470823 -0.216857   \n3    -0.033192 -0.042988 -0.237289 -0.597712  0.036943 -0.475983 -0.218702   \n4    -0.071789 -0.013570 -0.356966 -0.503781  0.103794 -0.478906 -0.219169   \n...        ...       ...       ...       ...       ...       ...       ...   \n2720  0.554206  0.035548  0.309859  0.511260  0.509747  0.480694  0.091120   \n2721  0.686508  0.095871  0.131412  0.511031  0.568968  0.489362  0.094142   \n2722  0.645852 -0.021438 -0.146584  0.475814  0.585492  0.494823  0.096659   \n2723 -1.017044 -0.166530 -0.393899  0.425741  0.563115  0.496723  0.098610   \n2724 -0.573783 -0.090431 -0.283347  0.386711  0.507279  0.495011  0.099941   \n\n            7         8         9         10        11        12  \n0    -0.298211 -0.178119 -0.177663  0.123194  0.129041  2.966678  \n1    -0.299652 -0.177326 -0.176424  0.123444  0.129086  2.966663  \n2    -0.300535 -0.176437 -0.175176  0.123693  0.129131  2.966647  \n3    -0.300854 -0.175452 -0.173919  0.123941  0.129175  2.966632  \n4    -0.300612 -0.174372 -0.172653  0.124189  0.129219  2.966616  \n...        ...       ...       ...       ...       ...       ...  \n2720  0.040725  0.000500 -0.142435  0.164101  0.107699  2.888278  \n2721  0.043953  0.000890 -0.142090  0.163991  0.107703  2.888260  \n2722  0.047117  0.001277 -0.141743  0.163879  0.107708  2.888242  \n2723  0.050218  0.001660 -0.141396  0.163766  0.107712  2.888224  \n2724  0.053253  0.002038 -0.141048  0.163652  0.107716  2.888206  \n\n[2725 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.079223</td>\n      <td>0.106928</td>\n      <td>-0.080420</td>\n      <td>-0.556599</td>\n      <td>-0.048314</td>\n      <td>-0.451930</td>\n      <td>-0.209208</td>\n      <td>-0.298211</td>\n      <td>-0.178119</td>\n      <td>-0.177663</td>\n      <td>0.123194</td>\n      <td>0.129041</td>\n      <td>2.966678</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.167664</td>\n      <td>0.061526</td>\n      <td>-0.061378</td>\n      <td>-0.577853</td>\n      <td>-0.040365</td>\n      <td>-0.462905</td>\n      <td>-0.213679</td>\n      <td>-0.299652</td>\n      <td>-0.177326</td>\n      <td>-0.176424</td>\n      <td>0.123444</td>\n      <td>0.129086</td>\n      <td>2.966663</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.110238</td>\n      <td>-0.007235</td>\n      <td>-0.102145</td>\n      <td>-0.605239</td>\n      <td>-0.012286</td>\n      <td>-0.470823</td>\n      <td>-0.216857</td>\n      <td>-0.300535</td>\n      <td>-0.176437</td>\n      <td>-0.175176</td>\n      <td>0.123693</td>\n      <td>0.129131</td>\n      <td>2.966647</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.033192</td>\n      <td>-0.042988</td>\n      <td>-0.237289</td>\n      <td>-0.597712</td>\n      <td>0.036943</td>\n      <td>-0.475983</td>\n      <td>-0.218702</td>\n      <td>-0.300854</td>\n      <td>-0.175452</td>\n      <td>-0.173919</td>\n      <td>0.123941</td>\n      <td>0.129175</td>\n      <td>2.966632</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.071789</td>\n      <td>-0.013570</td>\n      <td>-0.356966</td>\n      <td>-0.503781</td>\n      <td>0.103794</td>\n      <td>-0.478906</td>\n      <td>-0.219169</td>\n      <td>-0.300612</td>\n      <td>-0.174372</td>\n      <td>-0.172653</td>\n      <td>0.124189</td>\n      <td>0.129219</td>\n      <td>2.966616</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2720</th>\n      <td>0.554206</td>\n      <td>0.035548</td>\n      <td>0.309859</td>\n      <td>0.511260</td>\n      <td>0.509747</td>\n      <td>0.480694</td>\n      <td>0.091120</td>\n      <td>0.040725</td>\n      <td>0.000500</td>\n      <td>-0.142435</td>\n      <td>0.164101</td>\n      <td>0.107699</td>\n      <td>2.888278</td>\n    </tr>\n    <tr>\n      <th>2721</th>\n      <td>0.686508</td>\n      <td>0.095871</td>\n      <td>0.131412</td>\n      <td>0.511031</td>\n      <td>0.568968</td>\n      <td>0.489362</td>\n      <td>0.094142</td>\n      <td>0.043953</td>\n      <td>0.000890</td>\n      <td>-0.142090</td>\n      <td>0.163991</td>\n      <td>0.107703</td>\n      <td>2.888260</td>\n    </tr>\n    <tr>\n      <th>2722</th>\n      <td>0.645852</td>\n      <td>-0.021438</td>\n      <td>-0.146584</td>\n      <td>0.475814</td>\n      <td>0.585492</td>\n      <td>0.494823</td>\n      <td>0.096659</td>\n      <td>0.047117</td>\n      <td>0.001277</td>\n      <td>-0.141743</td>\n      <td>0.163879</td>\n      <td>0.107708</td>\n      <td>2.888242</td>\n    </tr>\n    <tr>\n      <th>2723</th>\n      <td>-1.017044</td>\n      <td>-0.166530</td>\n      <td>-0.393899</td>\n      <td>0.425741</td>\n      <td>0.563115</td>\n      <td>0.496723</td>\n      <td>0.098610</td>\n      <td>0.050218</td>\n      <td>0.001660</td>\n      <td>-0.141396</td>\n      <td>0.163766</td>\n      <td>0.107712</td>\n      <td>2.888224</td>\n    </tr>\n    <tr>\n      <th>2724</th>\n      <td>-0.573783</td>\n      <td>-0.090431</td>\n      <td>-0.283347</td>\n      <td>0.386711</td>\n      <td>0.507279</td>\n      <td>0.495011</td>\n      <td>0.099941</td>\n      <td>0.053253</td>\n      <td>0.002038</td>\n      <td>-0.141048</td>\n      <td>0.163652</td>\n      <td>0.107716</td>\n      <td>2.888206</td>\n    </tr>\n  </tbody>\n</table>\n<p>2725 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "0       0.079223\n1       0.167664\n2      -0.110238\n3      -0.033192\n4      -0.071789\n          ...   \n2720    0.554206\n2721    0.686508\n2722    0.645852\n2723   -1.017044\n2724   -0.573783\nName: 0, Length: 2725, dtype: float64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMF[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "13"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(IMF.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.07922266,  0.10692759, -0.0804196 , ...,  0.1231943 ,\n         0.1290414 ,  2.9666781 ],\n       [ 0.16766371,  0.06152574, -0.06137822, ...,  0.12344377,\n         0.12908604,  2.96666263],\n       [-0.11023805, -0.00723506, -0.10214502, ...,  0.12369274,\n         0.12913056,  2.96664714],\n       ...,\n       [ 0.64585228, -0.02143841, -0.14658354, ...,  0.16387918,\n         0.10770784,  2.88824221],\n       [-1.01704402, -0.16652988, -0.39389851, ...,  0.16376619,\n         0.10771212,  2.8882243 ],\n       [-0.57378275, -0.09043074, -0.28334668, ...,  0.16365182,\n         0.1077162 ,  2.88820643]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(IMF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "        0         1         2         3         4         5         6     \\\n0   0.079223  0.167664 -0.110238 -0.033192 -0.071789  0.108033  0.134707   \n1   0.106928  0.061526 -0.007235 -0.042988 -0.013570  0.037879  0.035816   \n2  -0.080420 -0.061378 -0.102145 -0.237289 -0.356966 -0.417812 -0.429407   \n3  -0.556599 -0.577853 -0.605239 -0.597712 -0.503781 -0.349364 -0.150800   \n4  -0.048314 -0.040365 -0.012286  0.036943  0.103794  0.178201  0.246775   \n5  -0.451930 -0.462905 -0.470823 -0.475983 -0.478906 -0.480113 -0.479859   \n6  -0.209208 -0.213679 -0.216857 -0.218702 -0.219169 -0.218225 -0.215877   \n7  -0.298211 -0.299652 -0.300535 -0.300854 -0.300612 -0.299823 -0.298503   \n8  -0.178119 -0.177326 -0.176437 -0.175452 -0.174372 -0.173199 -0.171934   \n9  -0.177663 -0.176424 -0.175176 -0.173919 -0.172653 -0.171378 -0.170094   \n10  0.123194  0.123444  0.123693  0.123941  0.124189  0.124437  0.124684   \n11  0.129041  0.129086  0.129131  0.129175  0.129219  0.129263  0.129307   \n12  2.966678  2.966663  2.966647  2.966632  2.966616  2.966601  2.966585   \n\n        7         8         9     ...      2715      2716      2717      2718  \\\n0   0.078454 -0.446654  0.487440  ... -0.351161  0.846940  0.625956 -0.859787   \n1  -0.033117 -0.028879  0.039557  ...  0.014994  0.022176  0.007857 -0.009390   \n2  -0.283760 -0.044046  0.358733  ... -0.311115 -0.288907 -0.097090  0.073601   \n3   0.131835  0.425981  0.619182  ...  0.177511  0.264896  0.384458  0.451318   \n4   0.293613  0.308375  0.286457  ... -0.106149  0.025202  0.159267  0.290102   \n5  -0.478459 -0.475966 -0.471793  ...  0.390725  0.414432  0.435634  0.453814   \n6  -0.212166 -0.207158 -0.200939  ...  0.069712  0.074761  0.079445  0.083747   \n7  -0.296668 -0.294338 -0.291528  ...  0.023755  0.027243  0.030690  0.034090   \n8  -0.170581 -0.169140 -0.167614  ... -0.001497 -0.001093 -0.000691 -0.000291   \n9  -0.168802 -0.167501 -0.166192  ... -0.144147 -0.143806 -0.143465 -0.143123   \n10  0.124930  0.125176  0.125421  ...  0.164631  0.164528  0.164423  0.164317   \n11  0.129351  0.129395  0.129439  ...  0.107672  0.107678  0.107683  0.107689   \n12  2.966569  2.966554  2.966538  ...  2.888368  2.888350  2.888332  2.888314   \n\n        2719      2720      2721      2722      2723      2724  \n0   0.039442  0.554206  0.686508  0.645852 -1.017044 -0.573783  \n1  -0.033962  0.035548  0.095871 -0.021438 -0.166530 -0.090431  \n2   0.301870  0.309859  0.131412 -0.146584 -0.393899 -0.283347  \n3   0.483405  0.511260  0.511031  0.475814  0.425741  0.386711  \n4   0.411381  0.509747  0.568968  0.585492  0.563115  0.507279  \n5   0.468858  0.480694  0.489362  0.494823  0.496723  0.495011  \n6   0.087645  0.091120  0.094142  0.096659  0.098610  0.099941  \n7   0.037436  0.040725  0.043953  0.047117  0.050218  0.053253  \n8   0.000106  0.000500  0.000890  0.001277  0.001660  0.002038  \n9  -0.142779 -0.142435 -0.142090 -0.141743 -0.141396 -0.141048  \n10  0.164210  0.164101  0.163991  0.163879  0.163766  0.163652  \n11  0.107694  0.107699  0.107703  0.107708  0.107712  0.107716  \n12  2.888296  2.888278  2.888260  2.888242  2.888224  2.888206  \n\n[13 rows x 2725 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2715</th>\n      <th>2716</th>\n      <th>2717</th>\n      <th>2718</th>\n      <th>2719</th>\n      <th>2720</th>\n      <th>2721</th>\n      <th>2722</th>\n      <th>2723</th>\n      <th>2724</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.079223</td>\n      <td>0.167664</td>\n      <td>-0.110238</td>\n      <td>-0.033192</td>\n      <td>-0.071789</td>\n      <td>0.108033</td>\n      <td>0.134707</td>\n      <td>0.078454</td>\n      <td>-0.446654</td>\n      <td>0.487440</td>\n      <td>...</td>\n      <td>-0.351161</td>\n      <td>0.846940</td>\n      <td>0.625956</td>\n      <td>-0.859787</td>\n      <td>0.039442</td>\n      <td>0.554206</td>\n      <td>0.686508</td>\n      <td>0.645852</td>\n      <td>-1.017044</td>\n      <td>-0.573783</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.106928</td>\n      <td>0.061526</td>\n      <td>-0.007235</td>\n      <td>-0.042988</td>\n      <td>-0.013570</td>\n      <td>0.037879</td>\n      <td>0.035816</td>\n      <td>-0.033117</td>\n      <td>-0.028879</td>\n      <td>0.039557</td>\n      <td>...</td>\n      <td>0.014994</td>\n      <td>0.022176</td>\n      <td>0.007857</td>\n      <td>-0.009390</td>\n      <td>-0.033962</td>\n      <td>0.035548</td>\n      <td>0.095871</td>\n      <td>-0.021438</td>\n      <td>-0.166530</td>\n      <td>-0.090431</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.080420</td>\n      <td>-0.061378</td>\n      <td>-0.102145</td>\n      <td>-0.237289</td>\n      <td>-0.356966</td>\n      <td>-0.417812</td>\n      <td>-0.429407</td>\n      <td>-0.283760</td>\n      <td>-0.044046</td>\n      <td>0.358733</td>\n      <td>...</td>\n      <td>-0.311115</td>\n      <td>-0.288907</td>\n      <td>-0.097090</td>\n      <td>0.073601</td>\n      <td>0.301870</td>\n      <td>0.309859</td>\n      <td>0.131412</td>\n      <td>-0.146584</td>\n      <td>-0.393899</td>\n      <td>-0.283347</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.556599</td>\n      <td>-0.577853</td>\n      <td>-0.605239</td>\n      <td>-0.597712</td>\n      <td>-0.503781</td>\n      <td>-0.349364</td>\n      <td>-0.150800</td>\n      <td>0.131835</td>\n      <td>0.425981</td>\n      <td>0.619182</td>\n      <td>...</td>\n      <td>0.177511</td>\n      <td>0.264896</td>\n      <td>0.384458</td>\n      <td>0.451318</td>\n      <td>0.483405</td>\n      <td>0.511260</td>\n      <td>0.511031</td>\n      <td>0.475814</td>\n      <td>0.425741</td>\n      <td>0.386711</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.048314</td>\n      <td>-0.040365</td>\n      <td>-0.012286</td>\n      <td>0.036943</td>\n      <td>0.103794</td>\n      <td>0.178201</td>\n      <td>0.246775</td>\n      <td>0.293613</td>\n      <td>0.308375</td>\n      <td>0.286457</td>\n      <td>...</td>\n      <td>-0.106149</td>\n      <td>0.025202</td>\n      <td>0.159267</td>\n      <td>0.290102</td>\n      <td>0.411381</td>\n      <td>0.509747</td>\n      <td>0.568968</td>\n      <td>0.585492</td>\n      <td>0.563115</td>\n      <td>0.507279</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.451930</td>\n      <td>-0.462905</td>\n      <td>-0.470823</td>\n      <td>-0.475983</td>\n      <td>-0.478906</td>\n      <td>-0.480113</td>\n      <td>-0.479859</td>\n      <td>-0.478459</td>\n      <td>-0.475966</td>\n      <td>-0.471793</td>\n      <td>...</td>\n      <td>0.390725</td>\n      <td>0.414432</td>\n      <td>0.435634</td>\n      <td>0.453814</td>\n      <td>0.468858</td>\n      <td>0.480694</td>\n      <td>0.489362</td>\n      <td>0.494823</td>\n      <td>0.496723</td>\n      <td>0.495011</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.209208</td>\n      <td>-0.213679</td>\n      <td>-0.216857</td>\n      <td>-0.218702</td>\n      <td>-0.219169</td>\n      <td>-0.218225</td>\n      <td>-0.215877</td>\n      <td>-0.212166</td>\n      <td>-0.207158</td>\n      <td>-0.200939</td>\n      <td>...</td>\n      <td>0.069712</td>\n      <td>0.074761</td>\n      <td>0.079445</td>\n      <td>0.083747</td>\n      <td>0.087645</td>\n      <td>0.091120</td>\n      <td>0.094142</td>\n      <td>0.096659</td>\n      <td>0.098610</td>\n      <td>0.099941</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.298211</td>\n      <td>-0.299652</td>\n      <td>-0.300535</td>\n      <td>-0.300854</td>\n      <td>-0.300612</td>\n      <td>-0.299823</td>\n      <td>-0.298503</td>\n      <td>-0.296668</td>\n      <td>-0.294338</td>\n      <td>-0.291528</td>\n      <td>...</td>\n      <td>0.023755</td>\n      <td>0.027243</td>\n      <td>0.030690</td>\n      <td>0.034090</td>\n      <td>0.037436</td>\n      <td>0.040725</td>\n      <td>0.043953</td>\n      <td>0.047117</td>\n      <td>0.050218</td>\n      <td>0.053253</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.178119</td>\n      <td>-0.177326</td>\n      <td>-0.176437</td>\n      <td>-0.175452</td>\n      <td>-0.174372</td>\n      <td>-0.173199</td>\n      <td>-0.171934</td>\n      <td>-0.170581</td>\n      <td>-0.169140</td>\n      <td>-0.167614</td>\n      <td>...</td>\n      <td>-0.001497</td>\n      <td>-0.001093</td>\n      <td>-0.000691</td>\n      <td>-0.000291</td>\n      <td>0.000106</td>\n      <td>0.000500</td>\n      <td>0.000890</td>\n      <td>0.001277</td>\n      <td>0.001660</td>\n      <td>0.002038</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.177663</td>\n      <td>-0.176424</td>\n      <td>-0.175176</td>\n      <td>-0.173919</td>\n      <td>-0.172653</td>\n      <td>-0.171378</td>\n      <td>-0.170094</td>\n      <td>-0.168802</td>\n      <td>-0.167501</td>\n      <td>-0.166192</td>\n      <td>...</td>\n      <td>-0.144147</td>\n      <td>-0.143806</td>\n      <td>-0.143465</td>\n      <td>-0.143123</td>\n      <td>-0.142779</td>\n      <td>-0.142435</td>\n      <td>-0.142090</td>\n      <td>-0.141743</td>\n      <td>-0.141396</td>\n      <td>-0.141048</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.123194</td>\n      <td>0.123444</td>\n      <td>0.123693</td>\n      <td>0.123941</td>\n      <td>0.124189</td>\n      <td>0.124437</td>\n      <td>0.124684</td>\n      <td>0.124930</td>\n      <td>0.125176</td>\n      <td>0.125421</td>\n      <td>...</td>\n      <td>0.164631</td>\n      <td>0.164528</td>\n      <td>0.164423</td>\n      <td>0.164317</td>\n      <td>0.164210</td>\n      <td>0.164101</td>\n      <td>0.163991</td>\n      <td>0.163879</td>\n      <td>0.163766</td>\n      <td>0.163652</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.129041</td>\n      <td>0.129086</td>\n      <td>0.129131</td>\n      <td>0.129175</td>\n      <td>0.129219</td>\n      <td>0.129263</td>\n      <td>0.129307</td>\n      <td>0.129351</td>\n      <td>0.129395</td>\n      <td>0.129439</td>\n      <td>...</td>\n      <td>0.107672</td>\n      <td>0.107678</td>\n      <td>0.107683</td>\n      <td>0.107689</td>\n      <td>0.107694</td>\n      <td>0.107699</td>\n      <td>0.107703</td>\n      <td>0.107708</td>\n      <td>0.107712</td>\n      <td>0.107716</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2.966678</td>\n      <td>2.966663</td>\n      <td>2.966647</td>\n      <td>2.966632</td>\n      <td>2.966616</td>\n      <td>2.966601</td>\n      <td>2.966585</td>\n      <td>2.966569</td>\n      <td>2.966554</td>\n      <td>2.966538</td>\n      <td>...</td>\n      <td>2.888368</td>\n      <td>2.888350</td>\n      <td>2.888332</td>\n      <td>2.888314</td>\n      <td>2.888296</td>\n      <td>2.888278</td>\n      <td>2.888260</td>\n      <td>2.888242</td>\n      <td>2.888224</td>\n      <td>2.888206</td>\n    </tr>\n  </tbody>\n</table>\n<p>13 rows × 2725 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(IMF)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def ceemdan_lstm(day):\n",
    "    mixer.init()\n",
    "    mixer.music.load('../../Resources/ExecuteTipAudio.mp3')\n",
    "\n",
    "    reshape1 = 0\n",
    "    IMF = pd.read_excel('../../Data/CEEMDAN/SingleFactor/558606/'+'CEEMDAN_58606 Station_'+day+'.xlsx',header=None)\n",
    "\n",
    "    imf_choose = 0\n",
    "    finally_rsult = []\n",
    "    finally_y_true = []\n",
    "\n",
    "    for imf_run in range(int(len(IMF.columns))):\n",
    "        # print(imf_run)\n",
    "        # print(IMF[0])\n",
    "        Single_Factor_IMF = IMF[imf_run]\n",
    "\n",
    "        print('--------------------------',imf_run,'--------------------------')\n",
    "        Single_Factor_IMF = np.array(Single_Factor_IMF)\n",
    "        # IMF_Input = np.transpose(Single_Factor_IMF)\n",
    "\n",
    "        df_IMF = pd.DataFrame(Single_Factor_IMF)\n",
    "        df_IMF.columns = ['ET0']\n",
    "\n",
    "        # print(df_IMF)\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        df = scaler.fit_transform(df_IMF)\n",
    "\n",
    "        sacler_data_len = len(df_IMF.columns)\n",
    "\n",
    "        #数据集划分\n",
    "        test_split = round(len(df) * 0.20)\n",
    "        print(test_split)\n",
    "        def splitData(var, per_test):\n",
    "            num_test = int(len(var) * per_test)\n",
    "            train_size = int(len(var) - num_test)\n",
    "            train_data = var[0:train_size]\n",
    "            test_data = var[train_size:train_size + num_test]\n",
    "            return train_data, test_data\n",
    "\n",
    "\n",
    "        df_training, df_testing = splitData(df, 0.2)\n",
    "        print('df_training.shape:',df_training.shape)\n",
    "        print('df_testing.shape:',df_testing.shape)\n",
    "\n",
    "        def createXY(data,n_past,n_steps_out):\n",
    "            dataX,dataY = list(),list()\n",
    "\n",
    "            for i in range(len(data)):\n",
    "                ## such as: len(data) = 19624\n",
    "                end_ix = i + n_past ## 0+3=3,1+3=4,...,19618+3=19621\n",
    "                out_end_ix = end_ix + n_steps_out ## 3+3=6,4+3=7,...,19621+3=19624\n",
    "                # print('out_end_ix:',out_end_ix)\n",
    "                '''\n",
    "                加入此处判断，使得最终的dataX和dataY中每行的数组长度一致，进而可以转换为array(数组),如若不加以限制，\n",
    "                则导致最后几次循环由于i是在len(data)范围内的，\n",
    "                但是由于out_end_ix=end_ix + n_steps_out,最终会超出data的数据范围，而导致其无法获取到数据,\n",
    "                加入dataY之后，最后几行的数据长度是和前面的数据长度不一致，最终导致无法进行array转换\n",
    "                '''\n",
    "                if out_end_ix > len(data): ## 6 < len(data),7<len(data),...,19623+3=19626>len(data)=19624\n",
    "                    # print(\"------------out_end_ix of end\",out_end_ix,'---------------')\n",
    "                    break\n",
    "\n",
    "                dataX.append(data[i:end_ix,0]) ## 0:3,0:7;1:4,0:7\n",
    "                # print('dataX:---------------')\n",
    "                # print(data[i:end_ix,0])\n",
    "                # print('dataX:---------------')\n",
    "                dataY.append(data[end_ix:out_end_ix,data.shape[1]-1]) ##3:6,6;4:7,6\n",
    "                # print('dataY:----------------------')\n",
    "                # print(data[end_ix:out_end_ix,data.shape[1]-1])\n",
    "                # print('dataY:----------------------')\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "\n",
    "        n_past=1\n",
    "        n_steps_out=1\n",
    "\n",
    "        trainX, trainY = createXY(df_training, n_past, n_steps_out)\n",
    "        testX, testY = createXY(df_testing, n_past, n_steps_out)\n",
    "\n",
    "        reshape1 = testY.shape[0]\n",
    "\n",
    "        print('trainX.Shape:----', trainX.shape)\n",
    "        print('trainY.shape:----', trainY.shape)\n",
    "        print('testX.shape:----', testX.shape)\n",
    "        print('testY.shape:----', testY.shape)\n",
    "\n",
    "        # trainY = trainY.reshape(-1)\n",
    "        # testY = testY.reshape(-1)\n",
    "        # print('trainY Shape 2 ---', trainY.shape)\n",
    "        # print('testY Shape 2 ---', testY.shape)\n",
    "        # trainX = np.expand_dims(trainX,axis=1)\n",
    "        # testX = np.expand_dims(testX,axis=1)\n",
    "        # print('expand_dims_trainX of shape',trainX.shape)\n",
    "        # print('expand_dims_testX of shape',testX.shape)\n",
    "\n",
    "        print('----------------------------',imf_run,'------------------------------')\n",
    "\n",
    "        def N_LSTM(batch_size=32, epochs=40):\n",
    "                model = Sequential()\n",
    "                model.add(LSTM(200, activation='relu', input_shape=(n_past, 1)))\n",
    "                # model.add(LSTM(100,activation='relu'))\n",
    "                #model.add(CuDNNLSTM(200, return_sequences=True, input_shape=(None, sacler_data_len)))\n",
    "\n",
    "                # model.add(LSTM(50, activation='relu'))\n",
    "                model.add(Dropout(0.1))\n",
    "                # model.add(LSTM(50, activation='relu'))\n",
    "                # model.add(Dropout(0.1))\n",
    "\n",
    "                # opm_adam = Adam(lr=0.01)  # 设置为您希望的学习率\n",
    "                # model.add(Dense(1))\n",
    "                ## Full connection layer: This is output shape(e.g. 3 Dimension that trainY.shape[1] = 3 represent for each output is 3 number)\n",
    "                model.add(Dense(1))\n",
    "\n",
    "                model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "                return model\n",
    "\n",
    "        N_LSTM_Model = KerasRegressor(N_LSTM, epochs=40, verbose=1, validation_data=(testX, testY))\n",
    "        # N_LSTM_Model = KerasRegressor(N_LSTM, epochs=40, verbose=1)\n",
    "\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "        # 定义超参数搜索范围\n",
    "        param_grid = {\n",
    "            # 'optimizer': ['adam'],\n",
    "            'batch_size': [128, 256],\n",
    "            'epochs': [20,30]\n",
    "            # 'batch_size': [64],\n",
    "            # 'epochs': [40]\n",
    "        }\n",
    "\n",
    "        # 执行网格搜索\n",
    "        grid = GridSearchCV(estimator=N_LSTM_Model, param_grid=param_grid, cv=2)\n",
    "        grid_result = grid.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "        best_params = grid_result.best_params_\n",
    "\n",
    "        best_params\n",
    "        print('prediction' , 'one' ,'day best_params:', best_params)\n",
    "\n",
    "        best_model = grid_result.best_estimator_\n",
    "\n",
    "        y_trues,prediction_result = Result_Ana(best_model, sacler_data_len,testX,testY,scaler)\n",
    "\n",
    "        print('prediction_result:',prediction_result)\n",
    "        finally_rsult.append(prediction_result)\n",
    "        print('lstm_y_turs:',y_trues)\n",
    "        finally_y_true.append(y_trues)\n",
    "\n",
    "        mixer.music.play()\n",
    "        time.sleep(1)\n",
    "        mixer.music.stop()\n",
    "    return reshape1,finally_y_true,finally_rsult,day"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def plot_save_true_prediction(reshape1,finally_y_true,finally_rsult,day):\n",
    "    # WL(Water Level(m)) For IMF1-IMF13-R True Value\n",
    "    finally_y_true\n",
    "    finally_ytrue_r = np.array(finally_y_true)\n",
    "    finally_ytrue_r\n",
    "    reshape2 = len(finally_ytrue_r)\n",
    "    reshape2\n",
    "    finally_column_names = []\n",
    "\n",
    "    for i in range(1, reshape2 + 1):\n",
    "        if i < (reshape2):\n",
    "            finally_column_names.append(\"IMF\" + str(i))\n",
    "        else:\n",
    "            finally_column_names.append(\"Residual\")\n",
    "    finally_column_names\n",
    "    ALL_Factor_ReIMFS_Yture_Result = np.transpose(finally_ytrue_r)\n",
    "    reshape1\n",
    "    ALL_Factor_ReIMFS_Yture_Result = ALL_Factor_ReIMFS_Yture_Result.reshape(reshape1, reshape2)\n",
    "    all_factor_ceemdan_true = pd.DataFrame(ALL_Factor_ReIMFS_Yture_Result, columns=finally_column_names)\n",
    "    all_factor_ceemdan_true\n",
    "    sums = all_factor_ceemdan_true.iloc[:, :].sum(axis=1)\n",
    "\n",
    "    # 将求和结果添加到DataFrame中作为新的一列\n",
    "    all_factor_ceemdan_true[\"True\"] = sums\n",
    "    # 保存为CSV文件\n",
    "    true_file_path = \"Result/CEEMDAN_Single_Factor/\"+day+\"_day_CEEMDAN_IMF\" + str(reshape2 - 1) + \"-LSTM_true.csv\"\n",
    "\n",
    "    all_factor_ceemdan_true.to_csv(true_file_path,\n",
    "                                   index=False)\n",
    "    # WL(Water Level(m)) IMF1-IMF13-R Prediction Value\n",
    "    finally_rsult\n",
    "    finally_r = np.array(finally_rsult)\n",
    "    finally_r\n",
    "    ALL_Factor_ReIMFS_Result = np.transpose(finally_r)\n",
    "    ALL_Factor_ReIMFS_Result = ALL_Factor_ReIMFS_Result.reshape(reshape1, reshape2)\n",
    "    finally_column_names\n",
    "    all_factor_ceemdan_prediction = pd.DataFrame(ALL_Factor_ReIMFS_Result, columns=finally_column_names)\n",
    "    all_factor_ceemdan_prediction\n",
    "    sums = all_factor_ceemdan_prediction.iloc[:, :].sum(axis=1)\n",
    "\n",
    "    # 将求和结果添加到DataFrame中作为新的一列\n",
    "    all_factor_ceemdan_prediction[\"Pred\"] = sums\n",
    "    prediction_file_path = \"Result/CEEMDAN_Single_Factor/\"+day+\"_day_CEEMDAN_IMF\" + str(reshape2 - 1) + \"-LSTM_prediction.csv\"\n",
    "    # 保存为CSV文件\n",
    "    all_factor_ceemdan_prediction.to_csv(prediction_file_path, index=False)\n",
    "    ### 验证最终结果\n",
    "    df = pd.read_csv(prediction_file_path)\n",
    "    df = df[['Pred']]\n",
    "    df\n",
    "\n",
    "    origin = pd.read_csv(true_file_path)\n",
    "    origin = origin[['True']]\n",
    "    # y_true = origin.iloc[:,len(origin.columns)-1]\n",
    "    y_true = origin\n",
    "    y_true\n",
    "    pred = df\n",
    "    y_true\n",
    "    pred\n",
    "    plt.plot(y_true, color='red', label='Real Value')\n",
    "    plt.plot(pred, color='blue', label='Pred Value')\n",
    "    plt.title('Prediction ET0 ')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Detail Value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "    from sklearn.metrics import r2_score as r2\n",
    "\n",
    "    print('MSE:', mse(y_true, pred))\n",
    "    print('MAE:', mae(y_true, pred))\n",
    "    print('R²:', r2(y_true, pred))\n",
    "    print('RMSE:', np.sqrt(mse(y_true, pred)))\n",
    "    print('pred_finally:', pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    days = ['one','three','five','seven','nine','ten','fifteen']\n",
    "    for day in days:\n",
    "       print('------------------',day ,'--------------------')\n",
    "       print('This is ',day,'-th for prediction et0')\n",
    "       reshape1,finally_y_true,finally_rsult,day = ceemdan_lstm(day)\n",
    "       print('This is ',day,'-th for prediction et0')\n",
    "       print('------------------',day,'---------------------')\n",
    "       plot_save_true_prediction(reshape1,finally_y_true,finally_rsult,day)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}